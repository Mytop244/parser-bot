import time, json, logging, asyncio, aiohttp, random, atexit, re
from datetime import timezone
from bs4 import BeautifulSoup
from dotenv import load_dotenv
load_dotenv()


def make_summarizer_prompt(article_text: str, language: str = "—Ä—É—Å—Å–∫–∏–π", max_tokens: int = 500) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π prompt –¥–ª—è –º–æ–¥–µ–ª–∏.
    –ó–∞–¥–∞—ë—Ç —Å—Ç–∏–ª—å, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–µ–≤ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ.
    """
    clean_text = article_text.replace("\n\n", "\n").strip()
    prompt = f"""
–ù–∞–ø–∏—à–∏ –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ–µ –∏ —á—ë—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ {language} —è–∑—ã–∫–µ.
- 2-3 –∞–±–∑–∞—Ü–∞, –∫–∞–∂–¥—ã–π –Ω–µ –±–æ–ª—å—à–µ 2-3 —Å—Ç—Ä–æ–∫
- –ë–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π –∏ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑
- –¢–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏ –≥–ª–∞–≤–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
- –ù–µ –ø–µ—Ä–µ—á–∏—Å–ª—è–π –¥–ª–∏–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤
- –î–æ–±–∞–≤—å 1-2 —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–æ–¥–∑–∏ –≤ –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ª–æ–∂–∏—Å—å –≤ {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤
- –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –Ω–µ –æ–±—Ä—ã–≤–∞–π –Ω–∞ –ø–æ–ª—É—Å–ª–æ–≤–µ

–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{clean_text}
"""
    return prompt.strip()

# ---------------- –ü–ê–†–ê–ú–ï–¢–†–´ ----------------
try:
    from main import (
        PARSER_MAX_TEXT_LENGTH,
        OLLAMA_TIMEOUT,
        MODEL_MAX_TOKENS,
        OLLAMA_MODEL,
        OLLAMA_MODEL_FALLBACK,
        AI_STUDIO_KEY,
        GEMINI_MODEL,
    )
except Exception:
    PARSER_MAX_TEXT_LENGTH = 10000
    OLLAMA_TIMEOUT = 180
    MODEL_MAX_TOKENS = 1200
    OLLAMA_MODEL = "gpt-oss:20b"
    OLLAMA_MODEL_FALLBACK = "gpt-oss:120b"
    AI_STUDIO_KEY = None
    GEMINI_MODEL = "gemini-2.5-flash"


# ---------------- –°–ï–°–°–ò–Ø (—É–ª—É—á—à–µ–Ω–∏–µ 1) ----------------
_AIO_CONN = None
_session = None

def get_session():
    global _AIO_CONN, _session
    if _AIO_CONN is None:
        _AIO_CONN = aiohttp.TCPConnector(limit=10)
    if _session is None or _session.closed:
        _session = aiohttp.ClientSession(connector=_AIO_CONN)
    return _session


@atexit.register
def _close_session():
    try:
        loop = asyncio.get_event_loop()
        if not _session.closed:
            loop.run_until_complete(_session.close())
    except Exception:
        pass


# ---------------- –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï (—É–ª—É—á—à–µ–Ω–∏–µ 2) ----------------
_semaphore = asyncio.Semaphore(3)

# ---------------- –ö–≠–® OLLAMA (—É–ª—É—á—à–µ–Ω–∏–µ 3) ----------------
_cache = {}


# ---------------- RETRY ----------------
async def async_retry(func, attempts=3, base=1.0, max_delay=30.0):
    delay = base
    for i in range(attempts):
        try:
            return await func()
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            if i == attempts - 1:
                raise
            jitter = random.uniform(0, delay * 0.5)
            logging.warning(f"‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {round(delay + jitter, 1)}—Å: {e}")
            await asyncio.sleep(min(delay + jitter, max_delay))
            delay *= 2


# ---------------- OLLAMA ----------------
async def summarize_ollama(text: str):
    if text in _cache:
        logging.info("‚ôªÔ∏è Ollama cache hit")
        return _cache[text], "cache"

    prompt_text = text[:PARSER_MAX_TEXT_LENGTH]
    prompt = make_summarizer_prompt(prompt_text, language="—Ä—É—Å—Å–∫–∏–π", max_tokens=MODEL_MAX_TOKENS)
    logging.info(f"üß† [OLLAMA INPUT] >>> {prompt_text[:5500]}")

    async def run_model(model_name: str):
        async with _semaphore:
            url = "http://127.0.0.1:11434/api/generate"
            payload = {"model": model_name, "prompt": prompt, "options": {"num_predict": MODEL_MAX_TOKENS}}
            start_time = time.time()
            session = get_session()
            try:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=OLLAMA_TIMEOUT)) as resp:
                    if resp.status != 200:
                        logging.error(f"‚ö†Ô∏è Ollama {model_name} HTTP {resp.status}")
                        return None, model_name

                    text_out, buffer, total = "", "", 0
                    MAX_STREAM_CHARS = PARSER_MAX_TEXT_LENGTH * 2
                    async for raw in resp.content:
                        if not raw:
                            continue
                        try:
                            chunk = raw.decode("utf-8")
                        except Exception:
                            continue
                        buffer += chunk
                        parts = buffer.splitlines()
                        if buffer and not buffer.endswith("\n"):
                            *lines, buffer = parts
                        else:
                            lines, buffer = parts, ""
                        for line in lines:
                            if not line.strip():
                                continue
                            try:
                                data = json.loads(line)
                            except Exception:
                                continue
                            frag = data.get("response", "")
                            text_out += frag
                            total += len(frag)
                            if total > MAX_STREAM_CHARS:
                                logging.warning("‚ö†Ô∏è Ollama stream truncated (limit reached)")
                                raise asyncio.CancelledError("stream too long")

                    out = text_out.strip()
                    if not out:
                        logging.warning(f"‚ö†Ô∏è Ollama ({model_name}) –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                        return None, model_name
                    elapsed = round(time.time() - start_time, 2)
                    logging.info(f"‚úÖ Ollama ({model_name}) –∑–∞ {elapsed} —Å–µ–∫")
                    _cache[text] = out
                    return out, model_name

            except asyncio.TimeoutError:
                logging.error(f"‚è∞ Ollama ({model_name}) —Ç–∞–π–º–∞—É—Ç")
            except asyncio.CancelledError:
                logging.warning(f"‚ö†Ô∏è Ollama ({model_name}) –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ –ª–∏–º–∏—Ç—É")
            except Exception as e:
                logging.error(f"‚ùå Ollama ({model_name}): {e}")
            return None, model_name

    try:
        result, used_model = await async_retry(lambda: run_model(OLLAMA_MODEL), attempts=3, base=1.5)
    except Exception:
        logging.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –º–æ–¥–µ–ª—å {OLLAMA_MODEL_FALLBACK}")
        result, used_model = await async_retry(lambda: run_model(OLLAMA_MODEL_FALLBACK), attempts=3, base=1.5)

    if not result:
        return prompt_text[:2000] + "...", "local-fallback"
    return result, used_model


# ---------------- GEMINI ----------------
async def summarize(text, max_tokens=200, retries=3):
    # –í—ã–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ HTML –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ –º–æ–¥–µ–ª—å
    def extract_relevant_paragraphs(html_text: str, max_paras: int = 6, min_len: int = 80) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–∏–º—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å—Ç–∞—Ç—å–∏ –¥–ª—è –º–æ–¥–µ–ª–∏."""
        if not html_text or "<" not in html_text:
            return html_text or ""
        soup = BeautifulSoup(html_text, "html.parser")
        paras = [p.get_text().strip() for p in soup.find_all("p") if len(p.get_text().strip()) >= min_len]
        return "\n\n".join(paras[:max_paras])

    text = extract_relevant_paragraphs(text)
    prompt_text = text[:PARSER_MAX_TEXT_LENGTH]
    prompt = make_summarizer_prompt(prompt_text, language="—Ä—É—Å—Å–∫–∏–π", max_tokens=max_tokens)

    if not AI_STUDIO_KEY:
        logging.debug(f"üß† [GEMINI INPUT] {prompt[:500]}...")
        logging.warning("‚ö†Ô∏è AI_STUDIO_KEY –Ω–µ –∑–∞–¥–∞–Ω, fallback –Ω–∞ Ollama")
        return await summarize_ollama(text)

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"maxOutputTokens": max_tokens or MODEL_MAX_TOKENS},
    }
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
    headers = {"x-goog-api-key": AI_STUDIO_KEY, "Content-Type": "application/json"}

    async def call_gemini():
        async with _semaphore:
            session = get_session()
            async with session.post(url, json=payload, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                body = await resp.text()
                if resp.status == 429:
                    logging.warning("‚ö†Ô∏è Gemini quota exceeded ‚Äî fallback to Ollama")
                    return None
                if resp.status >= 400:
                    raise aiohttp.ClientResponseError(
                        request_info=resp.request_info,
                        history=resp.history,
                        status=resp.status,
                        message=body,
                    )
                result = json.loads(body)
                candidates = result.get("candidates")
                if candidates:
                    parts = candidates[0].get("content", {}).get("parts", [])
                    if parts and "text" in parts[0]:
                        return parts[0]["text"]

    try:
        text_out = await async_retry(call_gemini, attempts=retries, base=2.0)
        if text_out:
            logging.info(f"‚úÖ Gemini OK ({GEMINI_MODEL}): {text_out}")
            return text_out.strip(), GEMINI_MODEL
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Gemini error: {e}")

    logging.error("‚ùå Gemini –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª, fallback –Ω–∞ Ollama")
    return await summarize_ollama(text)
