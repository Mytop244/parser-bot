import time, json, logging, asyncio, aiohttp, random, atexit
from datetime import timezone
from bs4 import BeautifulSoup
from dotenv import load_dotenv
load_dotenv()

# ---------------- –ü–ê–†–ê–ú–ï–¢–†–´ ----------------
try:
    from main import (
        PARSER_MAX_TEXT_LENGTH,
        OLLAMA_TIMEOUT,
        MODEL_MAX_TOKENS,
        OLLAMA_MODEL,
        OLLAMA_MODEL_FALLBACK,
        AI_STUDIO_KEY,
        GEMINI_MODEL,
    )
except Exception:
    PARSER_MAX_TEXT_LENGTH = 10000
    OLLAMA_TIMEOUT = 180
    MODEL_MAX_TOKENS = 1200
    OLLAMA_MODEL = "gpt-oss:20b"
    OLLAMA_MODEL_FALLBACK = "gpt-oss:120b"
    AI_STUDIO_KEY = None
    GEMINI_MODEL = "gemini-2.5-flash"


# ---------------- –°–ï–°–°–ò–Ø (—É–ª—É—á—à–µ–Ω–∏–µ 1) ----------------
_AIO_CONN = aiohttp.TCPConnector(limit=10)
_session = aiohttp.ClientSession(connector=_AIO_CONN)

def get_session():
    return _session

@atexit.register
def _close_session():
    try:
        loop = asyncio.get_event_loop()
        if not _session.closed:
            loop.run_until_complete(_session.close())
    except Exception:
        pass


# ---------------- –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï (—É–ª—É—á—à–µ–Ω–∏–µ 2) ----------------
_semaphore = asyncio.Semaphore(3)

# ---------------- –ö–≠–® OLLAMA (—É–ª—É—á—à–µ–Ω–∏–µ 3) ----------------
_cache = {}


# ---------------- RETRY ----------------
async def async_retry(func, attempts=3, base=1.0, max_delay=30.0):
    delay = base
    for i in range(attempts):
        try:
            return await func()
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            if i == attempts - 1:
                raise
            jitter = random.uniform(0, delay * 0.5)
            logging.warning(f"‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {round(delay + jitter, 1)}—Å: {e}")
            await asyncio.sleep(min(delay + jitter, max_delay))
            delay *= 2


# ---------------- OLLAMA ----------------
async def summarize_ollama(text: str):
    if text in _cache:
        logging.info("‚ôªÔ∏è Ollama cache hit")
        return _cache[text], "cache"

    prompt_text = text[:PARSER_MAX_TEXT_LENGTH]
    prompt = f"–ù–µ –¥–µ–ª–∞–π –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π. –°–¥–µ–ª–∞–π —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:\n{prompt_text}"
    logging.info(f"üß† [OLLAMA INPUT] >>> {prompt_text[:5500]}")

    async def run_model(model_name: str):
        async with _semaphore:
            url = "http://127.0.0.1:11434/api/generate"
            payload = {"model": model_name, "prompt": prompt, "options": {"num_predict": MODEL_MAX_TOKENS}}
            start_time = time.time()
            session = get_session()
            try:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=OLLAMA_TIMEOUT)) as resp:
                    if resp.status != 200:
                        logging.error(f"‚ö†Ô∏è Ollama {model_name} HTTP {resp.status}")
                        return None, model_name

                    text_out, buffer, total = "", "", 0
                    MAX_STREAM_CHARS = PARSER_MAX_TEXT_LENGTH * 2
                    async for raw in resp.content:
                        if not raw:
                            continue
                        try:
                            chunk = raw.decode("utf-8")
                        except Exception:
                            continue
                        buffer += chunk
                        parts = buffer.splitlines()
                        if buffer and not buffer.endswith("\n"):
                            *lines, buffer = parts
                        else:
                            lines, buffer = parts, ""
                        for line in lines:
                            if not line.strip():
                                continue
                            try:
                                data = json.loads(line)
                            except Exception:
                                continue
                            frag = data.get("response", "")
                            text_out += frag
                            total += len(frag)
                            if total > MAX_STREAM_CHARS:
                                logging.warning("‚ö†Ô∏è Ollama stream truncated (limit reached)")
                                raise asyncio.CancelledError("stream too long")

                    out = text_out.strip()
                    if not out:
                        logging.warning(f"‚ö†Ô∏è Ollama ({model_name}) –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                        return None, model_name
                    elapsed = round(time.time() - start_time, 2)
                    logging.info(f"‚úÖ Ollama ({model_name}) –∑–∞ {elapsed} —Å–µ–∫")
                    _cache[text] = out
                    return out, model_name

            except asyncio.TimeoutError:
                logging.error(f"‚è∞ Ollama ({model_name}) —Ç–∞–π–º–∞—É—Ç")
            except asyncio.CancelledError:
                logging.warning(f"‚ö†Ô∏è Ollama ({model_name}) –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ –ª–∏–º–∏—Ç—É")
            except Exception as e:
                logging.error(f"‚ùå Ollama ({model_name}): {e}")
            return None, model_name

    try:
        result, used_model = await async_retry(lambda: run_model(OLLAMA_MODEL), attempts=3, base=1.5)
    except Exception:
        logging.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –º–æ–¥–µ–ª—å {OLLAMA_MODEL_FALLBACK}")
        result, used_model = await async_retry(lambda: run_model(OLLAMA_MODEL_FALLBACK), attempts=3, base=1.5)

    if not result:
        return prompt_text[:2000] + "...", "local-fallback"
    return result, used_model


# ---------------- GEMINI ----------------
async def summarize(text, max_tokens=200, retries=3):
    text = BeautifulSoup(text, "html.parser").get_text() if text and "<" in text else (text or "")
    prompt_text = text[:PARSER_MAX_TEXT_LENGTH]
    prompt_text = f"–°–¥–µ–ª–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è, –¥–µ–ª–∏ –Ω–∞ –∞–±–∑–∞—Ü—ã:\n{prompt_text}"

    if not AI_STUDIO_KEY:
        logging.debug(f"üß† [GEMINI INPUT] {prompt_text[:500]}...")
        logging.warning("‚ö†Ô∏è AI_STUDIO_KEY –Ω–µ –∑–∞–¥–∞–Ω, fallback –Ω–∞ Ollama")
        return await summarize_ollama(text)

    payload = {
        "contents": [{"parts": [{"text": prompt_text}]}],
        "generationConfig": {"maxOutputTokens": max_tokens or MODEL_MAX_TOKENS},
    }
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
    headers = {"x-goog-api-key": AI_STUDIO_KEY, "Content-Type": "application/json"}

    async def call_gemini():
        async with _semaphore:
            session = get_session()
            async with session.post(url, json=payload, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                body = await resp.text()
                if resp.status == 429:
                    logging.warning("‚ö†Ô∏è Gemini quota exceeded ‚Äî fallback to Ollama")
                    return None
                if resp.status >= 400:
                    raise aiohttp.ClientResponseError(
                        request_info=resp.request_info,
                        history=resp.history,
                        status=resp.status,
                        message=body,
                    )
                result = json.loads(body)
                candidates = result.get("candidates")
                if candidates:
                    parts = candidates[0].get("content", {}).get("parts", [])
                    if parts and "text" in parts[0]:
                        return parts[0]["text"]

    try:
        text_out = await async_retry(call_gemini, attempts=retries, base=2.0)
        if text_out:
            logging.info(f"‚úÖ Gemini OK ({GEMINI_MODEL}): {text_out}")
            return text_out.strip(), GEMINI_MODEL
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Gemini error: {e}")

    logging.error("‚ùå Gemini –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª, fallback –Ω–∞ Ollama")
    return await summarize_ollama(text)
