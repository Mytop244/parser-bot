import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import os
import asyncio
import sys
import json
import logging
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from email.utils import parsedate_to_datetime
import time

# -------------------------------
# üîß –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
# -------------------------------
load_dotenv()

TIMEZONE = os.environ.get("TIMEZONE", "UTC")
os.environ['TZ'] = TIMEZONE
time.tzset()  # –ø—Ä–∏–º–µ–Ω—è–µ–º TZ

TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
CHAT_ID = os.environ.get("CHAT_ID")
RSS_URLS = os.environ.get("RSS_URLS", "").split(",")
NEWS_LIMIT = int(os.environ.get("NEWS_LIMIT", 5))
INTERVAL = int(os.environ.get("INTERVAL", 600))
SENT_LINKS_FILE = os.environ.get("SENT_LINKS_FILE", "sent_links.json")

if not TELEGRAM_TOKEN or not CHAT_ID:
    sys.exit("‚ùå –û—à–∏–±–∫–∞: TELEGRAM_TOKEN –∏–ª–∏ CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω—ã")

try:
    CHAT_ID = int(CHAT_ID)
except Exception:
    pass

bot = Bot(token=TELEGRAM_TOKEN)

# -------------------------------
# üîß –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π
# -------------------------------
os.makedirs("log", exist_ok=True)

log_formatter = logging.Formatter(
    "%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S"
)
log_formatter.converter = time.localtime  # –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è

log_filename = datetime.now().strftime("log/parser-%Y-%m-%d.log")

file_handler = TimedRotatingFileHandler(
    log_filename,
    when="midnight",
    interval=1,
    backupCount=7,
    encoding="utf-8",
    utc=False
)
file_handler.suffix = "%Y-%m-%d.log"
file_handler.setFormatter(log_formatter)

console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])

# -------------------------------
# üìÇ –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
# -------------------------------
if os.path.exists(SENT_LINKS_FILE):
    try:
        with open(SENT_LINKS_FILE, "r", encoding="utf-8") as f:
            sent_links = set(json.load(f))
    except Exception:
        sent_links = set()
else:
    sent_links = set()

def save_links():
    with open(SENT_LINKS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(sent_links), f, ensure_ascii=False, indent=2)

# -------------------------------
# üåê –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π
# -------------------------------
async def fetch_news(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                if resp.status != 200:
                    logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {resp.status}")
                    return []
                text = await resp.text()
    except Exception as e:
        logging.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ {url}: {e}")
        return []

    soup = BeautifulSoup(text, "lxml-xml")

    news_list = []
    for i in soup.find_all("item"):
        title = i.title.text if i.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        link = i.link.text if i.link else ""
        pub_date = None
        if i.pubDate:
            try:
                dt = parsedate_to_datetime(i.pubDate.text)
                if dt is not None:
                    # –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞—Ç—É –∫ UTC –∏ –¥–µ–ª–∞–µ–º –µ—ë offset-naive (–±–µ–∑ tzinfo)
                    if dt.tzinfo is None:
                        # –ï—Å–ª–∏ –Ω–µ—Ç tz, —Å—á–∏—Ç–∞–µ–º –¥–∞—Ç—É –≤ UTC
                        dt = dt.replace(tzinfo=timezone.utc)
                    dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
                    pub_date = dt
            except Exception:
                pass
        news_list.append((title, link, url, pub_date))

    return news_list

# -------------------------------
# ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
# -------------------------------
async def check_sources():
    logging.info("üîç –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    for url in RSS_URLS:
        if not url.strip():
            continue
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url.strip()) as resp:
                    if resp.status == 200:
                        logging.info(f"‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–æ—Å—Ç—É–ø–µ–Ω: {url}")
                    else:
                        logging.warning(f"‚ö†Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ {url} –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {resp.status}")
        except Exception as e:
            logging.error(f"‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫ {url} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

# -------------------------------
# üì© –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
# -------------------------------
async def send_news():
    all_news = []
    for url in RSS_URLS:
        if url.strip():
            news = await fetch_news(url.strip())
            all_news.extend(news)

    if not all_news:
        logging.warning("–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    logging.info(f"–ù–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π | –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(set(url for _, _, url, _ in all_news))}")

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (—Å–Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–µ)
    all_news.sort(key=lambda x: x[3] or datetime.min, reverse=True)

    sent_count = 0
    new_items = [n for n in all_news if n[1] not in sent_links]
    for title, link, source, pub_date in new_items[:NEWS_LIMIT]:

        if link in sent_links:
            continue
        try:
            date_str = pub_date.strftime("%Y-%m-%d %H:%M") if pub_date else "–±–µ–∑ –¥–∞—Ç—ã"
            await bot.send_message(
                chat_id=CHAT_ID,
                text=f"üì∞ {title}\n{link}\nüìÖ {date_str}\nüåç {source}"
            )
            sent_links.add(link)
            save_links()
            sent_count += 1
            logging.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {title} | –ò—Å—Ç–æ—á–Ω–∏–∫: {source} | –î–∞—Ç–∞: {date_str}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        await asyncio.sleep(1)

    logging.info(f"–í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {sent_count} –∏–∑ {NEWS_LIMIT}")

# -------------------------------
# üîÑ –¶–∏–∫–ª –≤–æ—Ä–∫–µ—Ä–∞
# -------------------------------
async def main():
    last_check = datetime.min
    while True:
        now = datetime.now()
        # —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if (now - last_check) > timedelta(days=1):
            await check_sources()
            last_check = now

        logging.info("–ù–∞—á–∞–ª–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")
        await send_news()
        logging.info(f"–°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {INTERVAL // 60} –º–∏–Ω")
        await asyncio.sleep(INTERVAL)

if __name__ == "__main__":
    asyncio.run(main())
