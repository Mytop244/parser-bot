import os, sys, json, time, asyncio, ssl, logging, tempfile, re, html, calendar, shutil
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
from collections import defaultdict, deque
from functools import partial
from dotenv import load_dotenv
import aiohttp, feedparser
import trafilatura
import hashlib
import random
import atexit
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.error import RetryAfter, TimedOut, NetworkError
from telegram.request import HTTPXRequest as Request
import aiosqlite
from logging.handlers import RotatingFileHandler

# --- Windows event loop policy ---
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

BASE_DIR = os.path.dirname(
    sys.executable if getattr(sys, 'frozen', False) else __file__
)

def fix_path(name: str) -> str:
    return os.path.join(BASE_DIR, name)

# ---- CONFIG / ENV ----
load_dotenv()

# --- Logging Setup (Rotating) ---
LOG_FILE = fix_path("parser.log")
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")

# –†–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤: –º–∞–∫—Å 5 –ú–ë, —Ö—Ä–∞–Ω–∏—Ç—å 3 —Ñ–∞–π–ª–∞
file_handler = RotatingFileHandler(LOG_FILE, maxBytes=5*1024*1024, backupCount=3, encoding="utf-8")
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.INFO)

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(formatter)

root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)
root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)

# --- Config Variables ---
CONCURRENCY = int(os.getenv("CONCURRENCY", "10"))
_network_semaphore = asyncio.Semaphore(CONCURRENCY)

BLOCKED_WORDS = [w.strip().lower() for w in os.getenv("BLOCKED_WORDS", "").split(",") if w.strip()]

DB_PATH = fix_path("bot_history.db")
META_FILE = fix_path("bot_meta.json")
STATE_JSON_PATH = fix_path("state.json") # –î–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏

SMART_PAUSE = os.getenv("SMART_PAUSE", "0") == "1"
SMART_PAUSE_MIN = int(os.getenv("SMART_PAUSE_MIN", "30"))
SMART_PAUSE_MAX = int(os.getenv("SMART_PAUSE_MAX", "60"))

STATE_DAYS_LIMIT = int(os.getenv("STATE_DAYS_LIMIT", "7"))
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
raw_chat = os.environ.get("CHAT_ID")
if raw_chat in (None, ""):
    CHAT_ID = None
else:
    try:
        CHAT_ID = int(raw_chat)
    except ValueError:
        raise RuntimeError(
            "‚ùå CHAT_ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º. "
            f"–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {repr(raw_chat)}"
        )


_env_rss = [u.strip() for u in os.environ.get("RSS_URLS", "").split(",") if u.strip()]
RSS_FILE = fix_path("rss.txt")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º fix_path, –∫–∞–∫ –¥–ª—è –ª–æ–≥–æ–≤ –∏ –ë–î
RSS_URLS = _env_rss
if os.path.exists(RSS_FILE):
    try:
        with open(RSS_FILE, 'r', encoding='utf-8') as f:
            RSS_URLS = [l.strip() for l in f if l.strip() and not l.strip().startswith('#')]
    except Exception:
        pass

NEWS_LIMIT = int(os.environ.get("NEWS_LIMIT", 5))
INTERVAL = int(os.environ.get("INTERVAL", 600))
DAYS_LIMIT = int(os.environ.get("DAYS_LIMIT", 1))
ROUND_ROBIN_MODE = int(os.environ.get("ROUND_ROBIN_MODE", 1))

GEMINI_KEYS = [k.strip() for k in os.getenv("GEMINI_KEYS", "").split(",") if k.strip()]
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "gpt-oss:20b")
OLLAMA_MODEL_FALLBACK = os.environ.get("OLLAMA_MODEL_FALLBACK", "gpt-oss:120b")
PARSER_MAX_TEXT_LENGTH = int(os.environ.get("PARSER_MAX_TEXT_LENGTH", "10000"))
MIN_ARTICLE_WORDS = int(os.environ.get("MIN_ARTICLE_WORDS", "50"))
MIN_TITLE_WORDS = int(os.environ.get("MIN_TITLE_WORDS", "5"))
MIN_TITLE_MATCHES = int(os.environ.get("MIN_TITLE_MATCHES", "3"))
OLLAMA_TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", 180))
MODEL_MAX_TOKENS = int(os.getenv("MODEL_MAX_TOKENS", 1200))
MODEL_TIMEOUT = int(os.getenv("MODEL_TIMEOUT", "120"))
GEMINI_PROMPT = os.getenv("GEMINI_PROMPT", "–°–¥–µ–ª–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è, –¥–µ–ª–∏ –Ω–∞ –∞–±–∑–∞—Ü—ã:\n{content}")
OLLAMA_PROMPT = os.getenv("OLLAMA_PROMPT", "–ù–µ –¥–µ–ª–∞–π –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π. –°–¥–µ–ª–∞–π —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:\n{content}")
GEMINI_MAX_TOKENS = int(os.getenv("GEMINI_MAX_TOKENS", 500))
MAX_ATTEMPTS = int(os.getenv("MAX_ATTEMPTS", 3))

GEMINI_BLOCK_MINUTES = int(os.getenv("GEMINI_BLOCK_MINUTES", "10"))
_gemini_key_lock = asyncio.Lock()
_blocked_keys = {}

OLLAMA_MAX_TOKENS = int(os.getenv("OLLAMA_MAX_TOKENS", 500))
ACTIVE_MODEL = os.getenv("ACTIVE_MODEL", GEMINI_MODEL)

BATCH_SIZE_SMALL = int(os.environ.get("BATCH_SIZE_SMALL", 5))
PAUSE_SMALL = int(os.environ.get("PAUSE_SMALL", 3))
BATCH_SIZE_MEDIUM = int(os.environ.get("BATCH_SIZE_MEDIUM", 15))
PAUSE_MEDIUM = int(os.environ.get("PAUSE_MEDIUM", 5))
BATCH_SIZE_LARGE = int(os.environ.get("BATCH_SIZE_LARGE", 25))
PAUSE_LARGE = int(os.environ.get("PAUSE_LARGE", 10))
SINGLE_MESSAGE_PAUSE = int(os.environ.get("SINGLE_MESSAGE_PAUSE", 1))

HEADER_TEMPLATE = os.getenv("HEADER_TEMPLATE", "<b>{title}</b>\nüì° <i>{source}</i> | üóì {date}\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
FOOTER_TEMPLATE = os.getenv("FOOTER_TEMPLATE", "\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nü§ñ <i>–ú–æ–¥–µ–ª—å: {model}</i>\nüîó <a href=\"{link}\">–ß–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç—å—é</a>")
BODY_PREFIX = os.getenv("BODY_PREFIX", "üí¨ ")
HTML_SAFE_LIMIT = 4096

APP_TZ_NAME = os.getenv("TIMEZONE", "UTC")
try:
    APP_TZ = ZoneInfo(APP_TZ_NAME)
except Exception:
    APP_TZ = timezone.utc

if not TELEGRAM_TOKEN or not CHAT_ID:
    raise RuntimeError("‚ùå TELEGRAM_TOKEN –∏–ª–∏ CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω—ã.")
if not RSS_URLS:
    raise RuntimeError("‚ùå RSS_URLS –Ω–µ –∑–∞–¥–∞–Ω—ã.")

# --- SSL ---
SSL_VERIFY = os.getenv("SSL_VERIFY", "1") not in ("0", "false", "False")
ssl_ctx = ssl.create_default_context()
if not SSL_VERIFY:
    ssl_ctx.check_hostname = False
    ssl_ctx.verify_mode = ssl.CERT_NONE

# --- GLOBAL CLASSES (DB & Meta) ---

class Database:
    def __init__(self, path):
        self.path = path
        self.conn = None

    async def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ SQLite –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        self.conn = await aiosqlite.connect(self.path)
        await self.conn.execute("PRAGMA journal_mode=WAL;")
        await self.conn.execute("""
            CREATE TABLE IF NOT EXISTS history (
                url TEXT NOT NULL,
                kind TEXT NOT NULL,
                timestamp INTEGER,
                PRIMARY KEY (url, kind)
            )
        """)
        await self.conn.execute("CREATE INDEX IF NOT EXISTS idx_ts ON history (timestamp);")
        await self.conn.commit()
        await self._migrate_legacy()

    async def close(self):
        if self.conn:
            await self.conn.close()

    async def _migrate_legacy(self):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ state.json"""
        if not os.path.exists(STATE_JSON_PATH): return
        logging.info("üîÑ –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä–æ–≥–æ state.json –≤ SQLite...")
        try:
            with open(STATE_JSON_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            for kind in ["seen", "sent"]:
                items = data.get(kind, {})
                if isinstance(items, dict):
                    for url, ts in items.items():
                        try:
                            ts_val = int(ts) if isinstance(ts, (int, float)) else int(time.time())
                        except Exception:
                            ts_val = int(time.time())
                        await self.conn.execute(
                            "INSERT OR IGNORE INTO history (url, kind, timestamp) VALUES (?, ?, ?)",
                            (url, kind, ts_val)
                        )
            await self.conn.commit()
            backup_name = STATE_JSON_PATH + ".bak"
            shutil.move(STATE_JSON_PATH, backup_name)
            logging.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. state.json -> {backup_name}")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")

    async def exists(self, kind: str, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–ø–∏—Å–∏"""
        if not self.conn: return False
        async with self.conn.execute("SELECT 1 FROM history WHERE url=? AND kind=?", (url, kind)) as cur:
            return await cur.fetchone() is not None

    async def add(self, kind: str, url: str, ts: int = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        if not self.conn: return
        if ts is None: ts = int(time.time())
        await self.conn.execute(
            "INSERT OR REPLACE INTO history (url, kind, timestamp) VALUES (?, ?, ?)", 
            (url, kind, int(ts))
        )
        await self.conn.commit()

    async def cleanup(self, days: int):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        if not self.conn: return
        cutoff = int(time.time() - (days * 86400))
        await self.conn.execute("DELETE FROM history WHERE timestamp < ?", (cutoff,))
        await self.conn.commit()
        logging.info(f"üßπ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞ (–∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π)")

class MetaManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ (–∫–ª—é—á–∏, –ø–∞—É–∑—ã) —á–µ—Ä–µ–∑ JSON"""
    def __init__(self, path):
        self.path = path
        self.data = {}
        self.load()

    def load(self):
        if os.path.exists(self.path):
            try:
                with open(self.path, "r", encoding="utf-8") as f: 
                    self.data = json.load(f)
            except: self.data = {}
        else: self.data = {}

    def save(self):
        try:
            fd, tmp = tempfile.mkstemp(prefix="meta_", dir=os.path.dirname(self.path))
            os.close(fd)
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(self.data, f, indent=2)
            os.replace(tmp, self.path)
        except Exception as e:
            logging.error(f"Meta save error: {e}")

    def get(self, key, default=None):
        return self.data.get(key, default)

    def set(self, key, value):
        self.data[key] = value
        self.save()

# --- INSTANCES ---
db = Database(DB_PATH)
meta_mgr = MetaManager(META_FILE)
_global_session = None
last_error = ""

# --- HELPERS ---

def set_last_error(val: str):
    global last_error
    last_error = val

async def get_session():
    global _global_session
    if _global_session is None or _global_session.closed:
        timeout = aiohttp.ClientTimeout(total=20, connect=5)
        connector = aiohttp.TCPConnector(limit=50, ssl=ssl_ctx, ttl_dns_cache=300)
        _global_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
    return _global_session

async def _limited(coro):
    async with _network_semaphore:
        return await coro

req = Request(connect_timeout=15, read_timeout=60, write_timeout=60)
bot = Bot(token=TELEGRAM_TOKEN, request=req)
_cache = {}

def split_text_safe(text: str, limit: int = HTML_SAFE_LIMIT) -> list[str]:
    parts = []
    if not text:
        return parts
    while len(text) > limit:
        pos = text.rfind("\n", 0, limit)
        if pos == -1:
            pos = text.rfind(" ", 0, limit)
        if pos == -1 or pos <= 0:
            pos = limit
        parts.append(text[:pos].strip())
        text = text[pos:].strip()
    if text:
        parts.append(text)
    return parts

def clean_text(text: str) -> str:
    if not text: return ""
    try:
        text = html.unescape(text)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º &quot; -> " –∏ &amp; -> &
        if "<" in text and ">" in text:
            text = BeautifulSoup(text, "html.parser").get_text()
    except Exception: pass
    return " ".join(text.split())

def parse_iso_utc(s):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç datetime –∏–ª–∏ —Å—Ç—Ä–æ–∫—É. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç datetime —Å tz=APP_TZ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç ISO, 'Z' –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
    """
    if isinstance(s, datetime):
        dt = s
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=APP_TZ)
        return dt.astimezone(APP_TZ)

    if not s:
        raise ValueError("empty date")

    s = s.strip()
    if s.endswith("Z"):
        s = s[:-1] + "+00:00"

    try:
        dt = datetime.fromisoformat(s)
    except Exception:
        dt = None
        for fmt in (
            "%d.%m.%Y, %H:%M",
            "%Y-%m-%d %H:%M",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%dT%H:%M:%S"
        ):
            try:
                dt = datetime.strptime(s, fmt)
                break
            except ValueError:
                continue
        if dt is None:
            raise ValueError(f"Invalid date: {s}")

    if dt.tzinfo:
        return dt.astimezone(APP_TZ)
    return dt.replace(tzinfo=APP_TZ)


def title_fingerprint(title: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π sha256-hex –æ—Ç–ø–µ—á–∞—Ç–æ–∫."""
    if not title:
        return ""
    norm = re.sub(r'\s+', ' ', re.sub(r'[^\w\s]', ' ', title.lower())).strip()
    if not norm:
        return ""
    h = hashlib.sha256(norm.encode("utf-8")).hexdigest()
    return "TITLEFP:" + h[:40]

def is_blocked_article(title: str, text: str, blocked_words: list | None = None) -> bool:
    bw = blocked_words if blocked_words is not None else BLOCKED_WORDS
    if not bw: return False
    combined = f"{title or ''} {text or ''}".casefold()
    for bad in bw:
        if not bad: continue
        try:
            if re.search(r'\b' + re.escape(bad) + r'\b', combined, flags=re.IGNORECASE):
                logging.info(f"üö´ Blocked by word: '{bad}'")
                return True
        except re.error:
            if bad in combined: return True
    return False

def validate_content_relevance(title: str, text: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
    """
    if not text or len(text) < 50:
        return False

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (–ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Ç.–¥.)
    def get_words(s):
        return set(w.lower() for w in re.findall(r'\w{4,}', s))

    title_words = get_words(title)
    text_words = get_words(text)

    if not title_words:
        return True # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É

    # –ò—â–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞. –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—â–µ–≥–æ —Å–ª–æ–≤–∞ >3 –±—É–∫–≤, —ç—Ç–æ –ª–µ–≤—ã–π —Ç–µ–∫—Å—Ç
    return bool(title_words.intersection(text_words))
def normalize_url(url: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç UTM-–º–µ—Ç–∫–∏, —Å–ª–µ—à–∏, www –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –µ–¥–∏–Ω–æ–º—É –ø—Ä–æ—Ç–æ–∫–æ–ª—É."""
    if not url: return ""
    try:
        u = urlparse(url)
        query = parse_qsl(u.query)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–µ –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
        clean_query = [(k, v) for k, v in query if not k.startswith(('utm_', 'fbclid', 'gclid', 'yclid', 'from', 'ref'))]
        sorted_query = urlencode(sorted(clean_query))
        path = u.path.rstrip('/') if len(u.path) > 1 else u.path
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏–º https –∏ —É–±–∏—Ä–∞–µ–º www –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        netloc = u.netloc.lower().replace('www.', '')
        
        return urlunparse(('https', netloc, path, u.params, sorted_query, u.fragment))
    except: return url.strip()


# --- NETWORK / PARSING ---

async def fetch_text_limited(response, max_bytes: int, ctx_url: str = ""):
    chunks, size = [], 0
    async for chunk in response.content.iter_chunked(8192):
        chunks.append(chunk)
        size += len(chunk)
        if size >= max_bytes: break
    try: return b"".join(chunks).decode(errors="ignore")
    except: return b"".join(chunks).decode("utf-8", errors="ignore")

async def fetch_url(session, url, head_only=False):
    try:
        headers = {"User-Agent": "NewsBot/1.0"}
        if head_only:
            async with session.head(url, ssl=ssl_ctx, headers=headers) as r:
                return url, "‚úÖ OK" if r.status == 200 else f"‚ö†Ô∏è HTTP {r.status}"
        async with session.get(url, ssl=ssl_ctx, headers=headers) as r:
            if r.status != 200: raise Exception(f"HTTP {r.status}")
            return await r.text()
    except Exception as e:
        return (url, f"‚ùå {e.__class__.__name__}") if head_only else (url, None)

async def fetch_and_check(session, url):
    logging.info(f"üîç Checking: {url}")
    res = await _limited(fetch_url(session, url))
    if isinstance(res, tuple) or not res:
        logging.warning(f"‚ö†Ô∏è Source failed: {url}")
        return None

    loop = asyncio.get_running_loop()
    try:
        feed = await loop.run_in_executor(None, feedparser.parse, res)
        entries = list(feed.entries)
    except Exception: return None

    if not entries: return None
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ (DAYS_LIMIT)
    limit_date = datetime.now(APP_TZ) - timedelta(days=DAYS_LIMIT)
    news = []
    
    for e in entries:
        try:
            if getattr(e, "published_parsed", None):
                pub = datetime.fromtimestamp(calendar.timegm(e.published_parsed), tz=APP_TZ)
                if pub < limit_date: continue
            else:
                pub = None # –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç, —Å—á–∏—Ç–∞–µ–º —Å–≤–µ–∂–µ–π (–∏–ª–∏ –∏–≥–Ω–æ—Ä–∏–º, –ø–æ –∂–µ–ª–∞–Ω–∏—é)
                
            summary = e.get("summary", "") or e.get("description", "") or ""
            news.append((
                clean_text(e.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")),  # –ß–∏—Å—Ç–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ä–∞–∑—É
                e.get("link", "").strip(),
                feed.feed.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫").strip(),
                summary,
                pub
            ))
        except Exception: continue
        
    return news

async def extract_article_text(url: str, ssl_context=None, max_length: int = 5000, session: aiohttp.ClientSession | None = None):
    ctx = ssl_context or ssl.create_default_context()
    headers = {"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0)"}
    
    html_text = ""
    try:
        sess = session or await get_session()
        async with sess.get(url, ssl=ctx, headers=headers) as r:
            if r.status != 200: return ""
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–æ 2–ú–ë –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞
            html_text = await fetch_text_limited(r, 2 * 1024 * 1024, url)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Fetch error {url}: {e}")
        return ""

    if not html_text: return ""
    
    # Trafilatura (CPU-bound) –∑–∞–ø—É—Å–∫–∞–µ–º –≤ executor
    try:
        loop = asyncio.get_running_loop()
        text = await loop.run_in_executor(
            None,
            partial(
                trafilatura.extract,
                html_text,
                include_comments=False,
                include_tables=False,
                deduplicate=True,
                target_language="ru"
            )
        )
    except Exception as e:
        logging.error(f"‚ùå Trafilatura error: {e}")
        text = None

    if not text:
        return ""
    
    # –û–±—Ä–µ–∑–∫–∞ –ø–æ –¥–ª–∏–Ω–µ
    if len(text) > max_length:
        text = text[:max_length].rsplit(" ", 1)[0]
        
    return text

# --- MODEL WRAPPERS ---

def _get_active_keys():
    now = time.time()
    return [k for k in GEMINI_KEYS if k not in _blocked_keys or _blocked_keys.get(k, 0) < now]

def _block_key_temporarily(key: str):
    _blocked_keys[key] = time.time() + GEMINI_BLOCK_MINUTES * 60

async def summarize_ollama(text: str):
    prompt = OLLAMA_PROMPT.format(content=text[:PARSER_MAX_TEXT_LENGTH])
    payload = {"model": OLLAMA_MODEL, "prompt": prompt, "stream": False, "options": {"num_predict": MODEL_MAX_TOKENS}}
    
    try:
        sess = await get_session()
        async with sess.post("http://127.0.0.1:11434/api/generate", json=payload, timeout=OLLAMA_TIMEOUT) as resp:
            if resp.status == 200:
                data = await resp.json()
                return data.get("response", "").strip(), OLLAMA_MODEL, "Local (Ollama)"
    except Exception as e:
        logging.error(f"Ollama error: {e}")
    return None, None, None

async def summarize_gemini(text: str, max_tokens: int | None = None):
    text = clean_text(text)
    prompt_text = GEMINI_PROMPT.format(content=text[:PARSER_MAX_TEXT_LENGTH])
    
    if not GEMINI_KEYS:
        return await summarize_ollama(text)

    eff_max = max_tokens or GEMINI_MAX_TOKENS
        
    # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è "–±–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏" –º–æ–¥–µ–ª–∏
    sys_instr = "–¢—ã ‚Äî API. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: 1 —Å—Ç—Ä–æ–∫–∞ - –ö–ª–∏–∫–±–µ–π—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫. 2 —Å—Ç—Ä–æ–∫–∞ –∏ –¥–∞–ª–µ–µ - –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ. –ù–µ –ø–∏—à–∏ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π —Ç–∏–ø–∞ '–í–æ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫'."

    payload = {
        "system_instruction": {"parts": [{"text": sys_instr}]},
        "contents": [{"parts": [{"text": prompt_text}]}],
        "generationConfig": {"maxOutputTokens": int(eff_max)},
    }
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
    session = await get_session()

    attempts = 0
    while attempts < MAX_ATTEMPTS:
        async with _gemini_key_lock:
            active = _get_active_keys()
            if not active:
                logging.warning("–í—Å–µ –∫–ª—é—á–∏ Gemini –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã, –ø—Ä–æ–±—É—é Ollama...")
                return await summarize_ollama(text)
            
            # –†–æ—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ MetaManager
            idx = int(meta_mgr.get("gemini_key_index", 0)) % len(active)
            key_to_use = active[idx]
            meta_mgr.set("gemini_key_index", (idx + 1) % len(active))

        try:
            async with session.post(url, headers={"Content-Type": "application/json", "x-goog-api-key": key_to_use}, json=payload, timeout=MODEL_TIMEOUT) as resp:
                if resp.status in (403, 429):
                    _block_key_temporarily(key_to_use)
                    attempts += 1
                    continue
                resp.raise_for_status()
                data = await resp.json()
                try:
                    res = data["candidates"][0]["content"]["parts"][0]["text"]
                    masked_key = f"{key_to_use[:4]}...{key_to_use[-4:]}"
                    return res.strip(), GEMINI_MODEL, masked_key
                except: return None, GEMINI_MODEL, None
        except Exception as e:
            logging.warning(f"Gemini error: {e}")
            attempts += 1
            await asyncio.sleep(2)
            
    return await summarize_ollama(text)

# --- UTILS ---

def sanitize_summary(s: str):
    if not s: return ""
    
    # 1. –°–Ω–∞—á–∞–ª–∞ —É–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ –∫–æ–¥–∞ (```), —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ –º–µ—à–∞–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥—É
    s = re.sub(r'```[\w]*\n?', '', s)

    # 2. –°–ø–∏—Å–æ–∫ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑ (RegEx)
    garbage = [
        # –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è (–ö–æ–Ω–µ—á–Ω–æ, –†–∞–∑—É–º–µ–µ—Ç—Å—è, –û–∫...)
        r'^(?:–∫–æ–Ω–µ—á–Ω–æ|—Ä–∞–∑—É–º–µ–µ—Ç—Å—è|–±–µ–∑—É—Å–ª–æ–≤–Ω–æ|–ø–æ–Ω—è–ª|—Ö–æ—Ä–æ—à–æ|–æ–∫|–ª–∞–¥–Ω–æ|–≥–æ—Ç–æ–≤–æ)[,!:.]?\s*',
        
        # –í–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–í–æ—Ç —Ä–µ–∑—é–º–µ, –î–µ—Ä–∂–∏ —Å–∞–º–º–∞—Ä–∏, –ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω...)
        r'^(?:–≤–æ—Ç|–¥–µ—Ä–∂–∏|–Ω–∏–∂–µ|–∑–¥–µ—Å—å)?\s*(?:–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω|–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω|–Ω–∞—Ö–æ–¥–∏—Ç—Å—è)?\s*(?:–≤–∞—à[–µ]?|–∫—Ä–∞—Ç–∫–æ–µ)?\s*(?:—Ä–µ–∑—é–º–µ|—Å–∞–º–º–∞—Ä–∏|—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ|–ø–µ—Ä–µ—Å–∫–∞–∑|–∏—Ç–æ–≥|–æ–±–∑–æ—Ä|–∞–Ω–∞–ª–∏–∑)(?:\s+(?:—Å—Ç–∞—Ç—å–∏|—Ç–µ–∫—Å—Ç–∞|–Ω–æ–≤–æ—Å—Ç–∏|–º–∞—Ç–µ—Ä–∏–∞–ª–∞))?[:\.]?\s*',
        
        # –ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è (–í —Å—Ç–∞—Ç—å–µ –≥–æ–≤–æ—Ä–∏—Ç—Å—è, –¢–µ–∫—Å—Ç –ø–æ—Å–≤—è—â–µ–Ω, –ê–≤—Ç–æ—Ä —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç...)
        r'^(?:–≤|–¥–∞–Ω–Ω(?:–∞—è|–æ–π)|—ç—Ç(?:–∞|–æ–π))\s+(?:—Å—Ç–∞—Ç—å(?:—å–µ|—è)|–Ω–æ–≤–æ—Å—Ç(?:—å|–∏)|–ø—É–±–ª–∏–∫–∞—Ü–∏(?:—è|–∏)|–º–∞—Ç–µ—Ä–∏–∞–ª(?:–µ)?)\s+(?:—Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç(?:—Å—è)?|–ø–æ–≤–µ—Å—Ç–≤—É–µ—Ç|–æ–ø–∏—Å—ã–≤–∞–µ—Ç|—Å–æ–æ–±—â–∞–µ—Ç|–ø–æ—Å–≤—è—â–µ–Ω[–∞–æ]?|—Å–æ–¥–µ—Ä–∂–∏—Ç|–≥–ª–∞—Å–∏—Ç|(?:—Ä–µ—á—å\s+)?–∏–¥–µ—Ç|–æ–±—Å—É–∂–¥–∞–µ—Ç(?:—Å—è)?)\s+(?:–æ|–ø—Ä–æ|—Ç–æ–º,?\s+—á—Ç–æ)?\s*',
        r'^(?:–∞–≤—Ç–æ—Ä|–∏—Å—Ç–æ—á–Ω–∏–∫)\s+(?:—Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç|—Å–æ–æ–±—â–∞–µ—Ç|–ø–∏—à–µ—Ç|–æ—Ç–º–µ—á–∞–µ—Ç|—É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç)\s+(?:–æ|—á—Ç–æ)?\s*',
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ Markdown, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—Å—Ç–∞–≤–∏—Ç—å (## –†–µ–∑—é–º–µ, **–ò—Ç–æ–≥**)
        r'^[\#\*]+\s*(?:—Ä–µ–∑—é–º–µ|–∏—Ç–æ–≥|—Å—É—Ç—å|–∫—Ä–∞—Ç–∫–æ|–≤—ã–≤–æ–¥|–æ—Å–Ω–æ–≤–Ω–æ–µ)(?:\s+–Ω–æ–≤–æ—Å—Ç–∏)?[\#\*]*[:\.]?\s*',
    ]

    # 3. –¶–∏–∫–ª–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ (—Å–Ω–∏–º–∞–µ–º —Å–ª–æ–∏ –º—É—Å–æ—Ä–∞ –æ–¥–∏–Ω –∑–∞ –¥—Ä—É–≥–∏–º)
    s = s.strip()
    while True:
        original = s
        for g in garbage:
            s = re.sub(g, '', s, flags=re.IGNORECASE).strip()
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–¥–∞ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è ‚Äî –≤—ã—Ö–æ–¥–∏–º
        if s == original:
            break

    if not s: return ""

    # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Markdown (—Å–ø–∏—Å–∫–∏, –∂–∏—Ä–Ω—ã–π, –∫—É—Ä—Å–∏–≤, —Å—Å—ã–ª–∫–∏)
    s = re.sub(r'(?m)^[\s]*[\*\-\u2013]\s+', '‚Ä¢ ', s)
    s = re.sub(r'\*\*([^\n*]+)\*\*', r'<b>\1</b>', s)
    s = re.sub(r'(?<!\*)\*([^\n*]+?)\*(?!\*)', r'<i>\1</i>', s)
    s = re.sub(r'`([^`\n]+?)`', r'<code>\1</code>', s)
    s = re.sub(r'\[([^\]]+?)\]\((https?://[^\s)]+)\)', r'<a href="\2">\1</a>', s)

    return s.strip()

def split_html_preserve(text: str, limit: int = HTML_SAFE_LIMIT - 200):
    parts, i, L = [], 0, len(text)
    while i < L:
        j = min(i + limit, L)
        lt, gt = text.rfind('<', i, j), text.rfind('>', i, j)
        if lt > gt: 
            next_gt = text.find('>', j)
            j = next_gt + 1 if next_gt != -1 and next_gt - i <= limit * 2 else i + limit
        parts.append(text[i:j])
        i = j
    return parts

async def send_with_retry(chat_id, text):
    for attempt in range(3):
        try:
            await bot.send_message(chat_id=chat_id, text=text, parse_mode="HTML")
            return
        except RetryAfter as e:
            await asyncio.sleep(getattr(e, 'retry_after', 5) + 1)
        except Exception as e:
            logging.warning(f"Send failed ({attempt}): {e}")
            await asyncio.sleep(5)

# --- MAIN LOGIC ---

async def send_news(session: aiohttp.ClientSession):
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–µ—Ä–µ–¥–∏ (–∏–∑ —Ñ–∞–π–ª–∞, –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏/–ø–æ–≤—Ç–æ—Ä–æ–≤)
    all_news = []
    # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Ü–∏–∫–ª–∞ ---
    stats = {
        "found": 0, "batch_unique": 0, "sent": 0,
        "dup_batch_url": 0, "dup_batch_title": 0,
        "dup_db_url": 0, "dup_db_title": 0, "blocked": 0, "irrelevant": 0, "fallback_rss": 0
    }
    if os.path.exists("news_queue.json"):
        try:
            with open("news_queue.json", "r", encoding="utf-8") as f:
                queued = json.load(f)
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Ñ–æ—Ä–º–∞—Ç —Å–æ–±–ª—é–¥–µ–Ω
            for item in queued:
                if len(item) >= 4:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º datetime
                    item_list = list(item)
                    if isinstance(item_list[-1], str):
                         try: item_list[-1] = parse_iso_utc(item_list[-1])
                         except: pass
                    all_news.append(tuple(item_list))
        except: pass

    # 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ RSS
    tasks = [fetch_and_check(session, url) for url in RSS_URLS]
    results = await asyncio.gather(*tasks)
    for r in results:
        if r: all_news.extend(r)

    if not all_news: return

    # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ DB
    stats["found"] = len(all_news)
    unique_news = []
    seen_urls_in_batch = set()
    seen_titles_in_batch = set()

    # ---- NEW: —Å–ø–∏—Å–∫–∏ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥—É–±–ª–µ–π ----
    dup_batch_url_samples = []
    dup_batch_title_samples = []    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É) –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    all_news.sort(key=lambda x: x[4] or datetime.min.replace(tzinfo=APP_TZ), reverse=True)

    for item in all_news:
        title = item[0]
        link = item[1]
                
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        clean_link = normalize_url(link)
        clean_title = clean_text(title).lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –î—É–±–ª–∏–∫–∞—Ç—ã –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ (–ø–æ —Å—Å—ã–ª–∫–µ –ò–õ–ò –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É)
        if clean_link in seen_urls_in_batch:
            stats["dup_batch_url"] += 1
            # –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
            if len(dup_batch_url_samples) < 5:
                dup_batch_url_samples.append(clean_link)
            continue
        if clean_title in seen_titles_in_batch:
            stats["dup_batch_title"] += 1
            if len(dup_batch_title_samples) < 5:
                dup_batch_title_samples.append(clean_title)
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ë–î (–ø–æ —á–∏—Å—Ç–æ–π —Å—Å—ã–ª–∫–µ –∏ –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—É)
        if await db.exists("sent", clean_link) or await db.exists("seen", clean_link):
            stats["dup_db_url"] += 1
            logging.info(f"üìö –î—É–±–ª–∏–∫–∞—Ç –≤ –ë–î (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL) ‚Üí {clean_link}")
            continue

        if clean_link != link and (await db.exists("sent", link) or await db.exists("seen", link)):
            stats["dup_db_url"] += 1
            logging.info(f"üìö –î—É–±–ª–∏–∫–∞—Ç –≤ –ë–î (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL) ‚Üí {link}")
            continue
        seen_urls_in_batch.add(clean_link)
        seen_titles_in_batch.add(clean_title)
        unique_news.append(item)
    
    stats["batch_unique"] = len(unique_news)  

    current_batch = unique_news[:NEWS_LIMIT]
    queue_rest = unique_news[NEWS_LIMIT:]
    sent_count = 0

    # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞
    for item in current_batch:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ç–µ–∂–∞
        if len(item) == 5: t, l, s, summary_raw, p = item
        else: t, l, s, p = item; summary_raw = ""

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π (–Ω–∞ —Å–ª—É—á–∞–π –≥–æ–Ω–∫–∏)
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π (–Ω–∞ —Å–ª—É—á–∞–π –≥–æ–Ω–∫–∏)
        norm_link = normalize_url(l)
        if await db.exists("sent", norm_link) or await db.exists("sent", l):
            continue

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ—Ç–ø–µ—á–∞—Ç–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–∞ ---
        fp = title_fingerprint(clean_text(t))
        if fp:
            if await db.exists("sent", fp) or await db.exists("seen", fp):
                stats["dup_db_title"] += 1
                logging.info(f"üîÅ –ü—Ä–æ–ø—É—â–µ–Ω–æ: –¥—É–±–ª–∏–∫–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ ‚Üí {t}")
                await db.add("seen", l)
                stats["blocked"] += 1
                logging.info(f"üö´ –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –ø–æ —Å–ª–æ–≤—É: {t}")      
                continue

        logging.info(f"‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞: {t}")
        
        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
        article_text = await _limited(extract_article_text(l, ssl_ctx, max_length=PARSER_MAX_TEXT_LENGTH, session=session))

        # --- –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –í–´–ë–û–† –ö–û–ù–¢–ï–ù–¢–ê ---
        clean_rss_summary = clean_text(summary_raw)
        is_relevant = validate_content_relevance(t, article_text)
        
        if is_relevant and len(re.findall(r'\w+', article_text)) >= MIN_ARTICLE_WORDS:
            # –¢–µ–∫—Å—Ç –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É –∏ –æ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π
            content = article_text
            logging.info(f"üìÑ –°—Ç–∞—Ç—å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ ‚Üí {l}")
        elif clean_rss_summary and len(clean_rss_summary) > 20:
            # –¢–µ–∫—Å—Ç –ø–ª–æ—Ö–æ–π/–ª–µ–≤—ã–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ RSS
            logging.info(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É. Fallback to RSS summary: {l}")
            content = clean_rss_summary
        else:
            # –°–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ –Ω–µ—Ç, –±–µ—Ä–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            content = t
            stats["irrelevant"] += 1
            logging.info(f"‚ö†Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –∏–ª–∏ –ø—É—Å—Ç–∞—è ‚Üí {l}")


        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        if is_blocked_article(t, content):
            await db.add("seen", l)
            continue

        # LLM Summary
        content = content[:PARSER_MAX_TEXT_LENGTH]
        active_lower = (ACTIVE_MODEL or "").lower()
        
        if "gemini" in active_lower:
            summ_text, used_model, key_info = await summarize_gemini(content)
        else:
            summ_text, used_model, key_info = await summarize_ollama(content)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—É–∑—ã/–æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏
        if used_model == "pause_1h" or (not summ_text and used_model is None):
            logging.warning("‚è∏Ô∏è –ù–µ—Ç –º–æ–¥–µ–ª–∏, –ø–∞—É–∑–∞ 1 —á–∞—Å.")
            meta_mgr.set("pause_until", int(time.time() + 3600))
            break # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –±–∞—Ç—á

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        final_summary = summ_text or ""
        display_title = t
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–¥–µ–ª–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç —Ç–µ–ª–∞ (–ø–æ –ø–µ—Ä–≤–æ–º—É –ø–µ—Ä–µ–Ω–æ—Å—É —Å—Ç—Ä–æ–∫–∏)
        if final_summary and "\n" in final_summary:
            parts = final_summary.split("\n", 1)
            # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è)
            if len(parts[0]) < 200:
                # –ß–∏—Å—Ç–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç Markdown –º–æ–¥–µ–ª–∏ (**Title**) –∏ HTML
                display_title = clean_text(parts[0].replace('*', '').strip())
                final_summary = parts[1].strip()

        local_time_str = (p or datetime.now(APP_TZ)).astimezone(APP_TZ).strftime("%d.%m.%Y, %H:%M")
        
        msg = (HEADER_TEMPLATE.format(title=html.escape(display_title.strip()), source=s, date=local_time_str) +
               BODY_PREFIX + sanitize_summary(final_summary) +
               FOOTER_TEMPLATE.format(model=used_model, link=html.escape(l, quote=True)))

        # –û—Ç–ø—Ä–∞–≤–∫–∞
        parts = split_html_preserve(msg)
        try:
            for part in parts:
                await send_with_retry(CHAT_ID, part)
                await asyncio.sleep(SINGLE_MESSAGE_PAUSE)
            
            # –£—Å–ø–µ—Ö -> –≤ –±–∞–∑—É
            ts = int(time.time())
            await db.add("sent", l, ts)
            await db.add("seen", l, ts)

            # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–ø–µ—á–∞—Ç–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ ---
            if fp:
                await db.add("sent", fp, ts)
                await db.add("seen", fp, ts)
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π –≤ –±—É–¥—É—â–µ–º
            if norm_link != l:
                await db.add("sent", norm_link, ts)
                await db.add("seen", norm_link, ts)

            sent_count += 1
            blocked_cnt = len([k for k, v in _blocked_keys.items() if v > time.time()])
            logging.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {t[:30]}... | üîë –ö–ª—é—á: {key_info} | ‚õî Blocked: {blocked_cnt}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats["sent"] += 1
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–∞ –æ—á–µ—Ä–µ–¥–∏ (queue_rest)
    if queue_rest:
        safe_queue = []
        for item in queue_rest:
            lst = list(item)
            # Date -> ISO string –¥–ª—è json
            if isinstance(lst[-1], datetime): lst[-1] = lst[-1].isoformat()
            safe_queue.append(lst)
        try:
            with open("news_queue.json", "w", encoding="utf-8") as f:
                json.dump(safe_queue, f, ensure_ascii=False)
        except: pass

    # –ü–∞—É–∑—ã
    if sent_count > 0:
        await asyncio.sleep(PAUSE_MEDIUM)
    elif SMART_PAUSE:
        # –£–º–Ω–∞—è –ø–∞—É–∑–∞ —Å –¥–∂–∏—Ç—Ç–µ—Ä–æ–º
        base = max(SMART_PAUSE_MIN, min(SMART_PAUSE_MAX, PAUSE_SMALL))
        wait = base + random.uniform(-2, 2)
        logging.info(f"üí§ Smart Pause: {wait:.1f}s")
        await asyncio.sleep(wait)
    # --- –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    logging.info("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logging.info("üìä –ò—Ç–æ–≥ —Ü–∏–∫–ª–∞:")
    logging.info(f"üîé –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {stats['found']}")
    logging.info(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤ –±–∞—Ç—á–µ: {stats['batch_unique']}")
    logging.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {stats['sent']}")
    logging.info(f"üîÅ –î—É–±–ª–µ–π –≤ –±–∞—Ç—á–µ (URL): {stats['dup_batch_url']}")
    logging.info(f"üîÅ –î—É–±–ª–µ–π –≤ –±–∞—Ç—á–µ (title): {stats['dup_batch_title']}")
    logging.info(f"üìö –î—É–±–ª–µ–π –≤ –ë–î (URL): {stats['dup_db_url']}")
    logging.info(f"üìö –î—É–±–ª–µ–π –≤ –ë–î (title-fp): {stats['dup_db_title']}")
    logging.info(f"üö´ –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤–æ–º: {stats['blocked']}")
    logging.info(f"‚ö†Ô∏è –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö/–ø—É—Å—Ç—ã—Ö: {stats['irrelevant']}")

    # --- NEW: –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥—É–±–ª–µ–π ---
    if stats["dup_batch_url"] > 0:
        logging.info("üîÅ –î—É–±–ª–∏–∫–∞—Ç—ã URL –≤ –±–∞—Ç—á–µ:")
        logging.info(f"    –í—Å–µ–≥–æ: {stats['dup_batch_url']}")
        if dup_batch_url_samples:
            logging.info("    –ü—Ä–∏–º–µ—Ä—ã:")
            for u in dup_batch_url_samples:
                logging.info(f"       ‚Ä¢ {u}")

    if stats["dup_batch_title"] > 0:
        logging.info("üîÅ –î—É–±–ª–∏–∫–∞—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –±–∞—Ç—á–µ:")
        logging.info(f"    –í—Å–µ–≥–æ: {stats['dup_batch_title']}")
        if dup_batch_title_samples:
            logging.info("    –ü—Ä–∏–º–µ—Ä—ã:")
            for t in dup_batch_title_samples:
                logging.info(f"       ‚Ä¢ {t}")

    logging.info("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")

async def check_sources():
    session = await get_session()
    for u in RSS_URLS:
        await fetch_url(session, u, head_only=True)

async def main():
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    await db.connect()
    # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, >7 –¥–Ω–µ–π)
    await db.cleanup(STATE_DAYS_LIMIT)
    
    last_check = datetime.now(APP_TZ)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞ —Å–µ—Å—Å–∏–∏
    if "ollama" in (ACTIVE_MODEL or "").lower():
        t_out = aiohttp.ClientTimeout(total=None)
        base_timeout = None
    else:
        t_out = aiohttp.ClientTimeout(total=INTERVAL)
        base_timeout = INTERVAL

    async with aiohttp.ClientSession(timeout=t_out) as session:
        logging.info("üöÄ Bot started. Waiting for tasks...")
        try:
            while True:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–∞—É–∑—ã
                pause_until = meta_mgr.get("pause_until")
                if pause_until:
                    rem = pause_until - time.time()
                    if rem > 0:
                        logging.info(f"‚è∏Ô∏è Pause active for {int(rem)}s")
                        await asyncio.sleep(rem)
                        meta_mgr.set("pause_until", None)
                        continue
                    else:
                        meta_mgr.set("pause_until", None)

                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                now = datetime.now(APP_TZ)
                if (now - last_check) > timedelta(days=1):
                    await check_sources()
                    last_check = now

                # –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
                logging.info("üîÑ Checking RSS...")
                try:
                    if base_timeout:
                        await asyncio.wait_for(send_news(session), timeout=base_timeout)
                    else:
                        await send_news(session)
                except asyncio.TimeoutError:
                    logging.warning("‚è∞ Timeout in main loop")
                except Exception as e:
                    logging.exception(f"‚ùå Main loop exception: {e}")

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
                await asyncio.sleep(INTERVAL)

        except KeyboardInterrupt:
            logging.info("üõë Bot stopping...")
        finally:
            await db.close()

@atexit.register
def _cleanup():
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞
    pass

if __name__ == "__main__":
    asyncio.run(main())